tool: hakrawler
tags: [web, crawler, recon, reconnaissance, pentest, security, enumeration, golang]

actions:
  # === BASIC USAGE ===
  - title: hakrawler - simple crawl
    desc: |
      Perform a basic crawl on a single target URL using default settings.
      This is the most common entry point for reconnaissance to identify links and JavaScript files.
      The tool reads the target from standard input, making it easy to pipe from other tools.
    command: "echo {{url}} | hakrawler"

  - title: hakrawler - multiple target crawl
    desc: |
      Crawl multiple target URLs provided from a file.
      This is highly efficient for large-scale reconnaissance across multiple domains or endpoints.
      Each line in the input file should contain a valid URL starting with http or https.
    command: "cat {{url_list_file}} | hakrawler"

  # === CRAWLER CONFIGURATION ===
  - title: hakrawler - set crawl depth
    desc: |
      Define the maximum depth the crawler should navigate into the site structure.
      The default depth is 2, but increasing it allows for deeper discovery of nested pages.
      Be cautious as higher depth values significantly increase the time and resource consumption.
    command: "echo {{url}} | hakrawler -d {{depth|3}}"

  - title: hakrawler - include subdomains
    desc: |
      Allow the crawler to follow and report links that belong to subdomains of the target.
      By default, hakrawler stays within the scope of the exact domain provided.
      This flag is essential when the main site redirects to a 'www' or other functional subdomains.
    command: "echo {{url}} | hakrawler -subs"

  - title: hakrawler - unique urls only
    desc: |
      Filter the output to show only unique URLs discovered during the crawl.
      This reduces noise in the output by removing duplicate links found in different parts of the site.
      It is particularly useful when generating wordlists or target lists for further scanning.
    command: "echo {{url}} | hakrawler -u"

  # === NETWORK & AUTHENTICATION ===
  - title: hakrawler - proxy configuration
    desc: |
      Route all crawler traffic through a specified HTTP or SOCKS proxy.
      This is useful for debugging traffic through tools like Burp Suite or for bypassing IP-based restrictions.
      Ensure the proxy URL is correctly formatted with the protocol and port.
    command: "echo {{url}} | hakrawler -proxy {{proxy_url|http://127.0.0.1:8080}}"

  - title: hakrawler - custom headers
    desc: |
      Send custom HTTP headers with every request initiated by the crawler.
      This is required for crawling sites that need authentication via Cookies or Authorization tokens.
      Multiple headers must be separated by two semi-colons (;;).
    command: "echo {{url}} | hakrawler -h \"{{headers|Cookie: session=123;;User-Agent: Mozilla/5.0}}\""

  - title: hakrawler - insecure tls
    desc: |
      Disable TLS certificate verification for HTTPS targets.
      Use this when dealing with internal environments or servers using self-signed certificates.
      Note that this decreases security as it makes the connection vulnerable to MITM attacks.
    command: "echo {{url}} | hakrawler -insecure"

  # === OUTPUT & PERFORMANCE ===
  - title: hakrawler - json output
    desc: |
      Generate the output in structured JSON format for automated processing.
      The JSON output contains detailed information including the source type and discovery location.
      This is the preferred format for integrating hakrawler into automated bug bounty pipelines.
    command: "echo {{url}} | hakrawler -json"

  - title: hakrawler - show source and link origin
    desc: |
      Display where the URL was found (e.g., href, script) and the exact link that contained it.
      This provides context for each discovered endpoint, which is crucial for manual verification.
      Using the -s and -w flags together provides the most detailed human-readable output.
    command: "echo {{url}} | hakrawler -s -w"

  - title: hakrawler - performance tuning
    desc: |
      Adjust the number of threads and the timeout to optimize crawling speed.
      Increasing threads allows for faster crawling but may cause rate limiting or server stress.
      The timeout ensures that the crawler does not hang indefinitely on slow or unresponsive endpoints.
    command: "echo {{url}} | hakrawler -t {{threads|10}} -timeout {{timeout_seconds|5}}"

  # === ADVANCED CHAINING ===
  - title: hakrawler - combined recon pipeline
    desc: |
      Example of chaining hakrawler with other tools like httpx for a complete workflow.
      This command takes a domain, finds its subdomains, checks for active web services, and then crawls them.
      It demonstrates the power of hakrawler in a modern Linux-based reconnaissance toolchain.
    command: "echo {{domain}} | haktrails subdomains | httpx | hakrawler -subs -u"