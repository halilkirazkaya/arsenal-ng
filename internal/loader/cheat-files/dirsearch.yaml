tool: dirsearch
tags: [web, pentest, reconnaissance, enumeration, scanner, security, brute-force]

actions:
  # === BASIC USAGE ===
  - title: dirsearch - basic scan
    desc: |
      Perform a basic directory search against a single target URL using specific extensions.
      This is the most common starting point for discovering hidden files and directories on a web server.
      Providing extensions like php, asp, or html helps the tool narrow down the search to relevant file types for the target environment.
      Always ensure you have permission to scan the target to avoid legal issues.
    command: "dirsearch -u {{url}} -e {{extensions|php,html,js,txt}}"

  - title: dirsearch - scan list of urls
    desc: |
      Scan multiple target URLs simultaneously by providing a text file containing one URL per line.
      This is highly efficient when performing reconnaissance across a large infrastructure or multiple subdomains.
      The tool will iterate through each URL using the same configuration and extension settings provided.
      Be cautious of system resource usage when scanning a large list of targets with many threads.
    command: "dirsearch -l {{url_file_path}} -e {{extensions|*}}"

  # === DICTIONARY & WORDLIST SETTINGS ===
  - title: dirsearch - custom wordlist
    desc: |
      Use a specific wordlist for the directory brute-forcing process instead of the default internal list.
      This is useful when you have technology-specific wordlists or high-quality discovery lists like SecLists.
      You can provide multiple wordlists separated by commas to combine different sets of potential paths.
      Ensure the wordlist path is correct and accessible by the user running the command.
    command: "dirsearch -u {{url}} -e {{extensions|*}} -w {{wordlist_path|/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt}}"

  - title: dirsearch - prefixes and suffixes
    desc: |
      Append custom prefixes or suffixes to every entry in the wordlist during the scan.
      This is particularly helpful for finding backup files (e.g., .bak, .old) or environment-specific hidden folders.
      Prefixes are added to the beginning of the wordlist entry, while suffixes are added to the end.
      Using this feature can significantly increase the total number of requests sent to the server.
    command: "dirsearch -u {{url}} -e {{extensions|php}} --prefixes {{prefixes|_,backup-}} --suffixes {{suffixes|~,.swp}}"

  # === FILTERING & STATUS CODES ===
  - title: dirsearch - filter status codes
    desc: |
      Include or exclude specific HTTP status codes from the results to reduce noise.
      By default, dirsearch shows many codes, but you might only be interested in 200 (OK) or 301 (Redirect).
      Excluding codes like 403 (Forbidden) or 500 (Internal Server Error) helps focus on accessible resources.
      You can specify individual codes or ranges, such as 200-299, separated by commas.
    command: "dirsearch -u {{url}} -e {{extensions|*}} -i {{include_codes|200,301}} -x {{exclude_codes|400,403,500-599}}"

  - title: dirsearch - filter by size or text
    desc: |
      Exclude responses based on their body size or the presence of specific text/regex.
      This is extremely useful for filtering out "fake" 200 OK pages or custom error pages that all return the same size.
      If a server returns a specific message for all non-existent pages, you can use --exclude-text to hide them.
      This helps in identifying unique responses that might actually represent valid hidden files.
    command: "dirsearch -u {{url}} -e {{extensions|*}} --exclude-sizes {{sizes|0B,123B}} --exclude-text {{text|'Page Not Found'}}"

  # === RECURSION ===
  - title: dirsearch - recursive scan
    desc: |
      Automatically initiate a new scan for every directory discovered during the initial brute-force.
      This allows for deep exploration of the web application's directory structure without manual intervention.
      You should set a maximum recursion depth to prevent the scan from running indefinitely on complex sites.
      Recursive scanning significantly increases the time required for the scan to complete.
    command: "dirsearch -u {{url}} -e {{extensions|php}} -r -R {{max_depth|3}}"

  # === REQUEST CUSTOMIZATION ===
  - title: dirsearch - authenticated scan
    desc: |
      Perform a scan using specific authentication credentials or session cookies.
      This is essential for discovering files and directories located within authenticated areas of a web application.
      You can specify the authentication type such as basic, digest, or bearer depending on the server's requirements.
      Make sure the session remains active; if the cookie expires, you will likely only see 302 or 403 responses.
    command: "dirsearch -u {{url}} -e {{extensions|php}} --cookie \"{{cookie_string}}\" --auth {{credentials|user:password}} --auth-type {{auth_type|basic}}"

  - title: dirsearch - custom http method and headers
    desc: |
      Change the default GET method to other HTTP verbs like POST, PUT, or HEAD.
      Adding custom headers can help bypass some Web Application Firewalls (WAFs) or simulate specific client environments.
      For example, adding an 'X-Forwarded-For' header might trick the server into thinking the request is coming from a trusted local IP.
      This is also useful for API fuzzing where specific 'Content-Type' or 'Accept' headers are mandatory.
    command: "dirsearch -u {{url}} -e {{extensions|*}} -m {{method|POST}} -H \"{{header_name|X-Forwarded-For}}: {{header_value|127.0.0.1}}\""

  # === CONNECTION & PERFORMANCE ===
  - title: dirsearch - performance tuning
    desc: |
      Adjust the number of threads and the delay between requests to optimize the scan's speed and stealth.
      Increasing threads will speed up the process but may cause a Denial of Service (DoS) or trigger rate-limiting.
      Adding a delay is recommended when dealing with targets that have aggressive Intrusion Detection Systems (IDS).
      A balanced approach is usually best to ensure accuracy while maintaining a reasonable scanning speed.
    command: "dirsearch -u {{url}} -e {{extensions|*}} -t {{threads|50}} --delay {{delay|0.1}} --max-rate {{rate|20}}"

  - title: dirsearch - scan via proxy or tor
    desc: |
      Route all scanning traffic through a proxy server or the Tor network to mask your original IP address.
      This is a common practice in red teaming to avoid being blocked by the target's firewall or to bypass Geo-IP restrictions.
      When using the --tor flag, ensure the Tor service is running locally on your machine.
      Using proxies may slow down the scan significantly depending on the proxy's bandwidth and latency.
    command: "dirsearch -u {{url}} -e {{extensions|*}} --proxy {{proxy_url|http://127.0.0.1:8080}} --tor"

  # === OUTPUT & LOGGING ===
  - title: dirsearch - save output to file
    desc: |
      Export the discovery results into a formatted file for documentation or further automated analysis.
      Supported formats include JSON, XML, HTML, CSV, and Markdown, making it easy to integrate with other tools.
      Saving results is highly recommended for long-running scans to ensure data is not lost if the terminal closes.
      The JSON format is particularly useful for piping results into custom scripts or databases.
    command: "dirsearch -u {{url}} -e {{extensions|*}} -o {{output_path|results.json}} --format {{format|json}}"

  # === ADVANCED FEATURES ===
  - title: dirsearch - crawl for new paths
    desc: |
      Enable the crawler to find new paths by parsing the HTML responses of discovered pages.
      This combines traditional wordlist-based brute-forcing with spidering capabilities for better coverage.
      The crawler will extract links from tags like <a>, <script>, and <img> to find valid endpoints.
      This feature is effective for discovering paths that are not present in standard wordlists.
    command: "dirsearch -u {{url}} -e {{extensions|*}} --crawl"